{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Optim initial step length guess"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This example shows how to use the initial step length procedures\n",
    "with [Optim](https://github.com/JuliaNLSolvers/Optim.jl).  We solve\n",
    "the Rosenbrock problem with two different procedures.\n",
    "\n",
    "First, run `Newton` with the (default) initial guess and line search procedures."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": " * Status: success\n\n * Candidate solution\n    Minimizer: [1.00e+00, 1.00e+00]\n    Minimum:   1.109336e-29\n\n * Found with\n    Algorithm:     Newton's Method\n    Initial Point: [-1.20e+00, 1.00e+00]\n\n * Convergence measures\n    |x - x'|               = 1.13e-08 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.13e-08 ≰ 0.0e+00\n    |f(x) - f(x')|         = 7.05e-16 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 6.35e+13 ≰ 0.0e+00\n    |g(x)|                 = 6.66e-15 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   1  (vs limit Inf)\n    Iterations:    23\n    f(x) calls:    71\n    ∇f(x) calls:   71\n    ∇²f(x) calls:  23\n"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "using Optim, LineSearches\n",
    "import OptimTestProblems.MultivariateProblems\n",
    "UP = MultivariateProblems.UnconstrainedProblems\n",
    "prob = UP.examples[\"Rosenbrock\"]\n",
    "\n",
    "algo_st = Newton(alphaguess = InitialStatic(), linesearch = HagerZhang())\n",
    "res_st = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, method=algo_st)"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We can now try with the initial step length guess from Hager and Zhang."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": " * Status: success\n\n * Candidate solution\n    Minimizer: [1.00e+00, 1.00e+00]\n    Minimum:   1.719626e-19\n\n * Found with\n    Algorithm:     Newton's Method\n    Initial Point: [-1.20e+00, 1.00e+00]\n\n * Convergence measures\n    |x - x'|               = 4.16e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 4.16e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 6.27e-10 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 3.65e+09 ≰ 0.0e+00\n    |g(x)|                 = 6.63e-10 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    24\n    f(x) calls:    61\n    ∇f(x) calls:   38\n    ∇²f(x) calls:  24\n"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "algo_hz = Newton(alphaguess = InitialHagerZhang(α0=1.0), linesearch = HagerZhang())\n",
    "res_hz = Optim.optimize(prob.f, prob.g!, prob.h!, prob.initial_x, method=algo_hz)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "From the result we see that this has reduced the number of function and gradient calls, but increased the number of iterations."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.4"
  },
  "kernelspec": {
   "name": "julia-1.0",
   "display_name": "Julia 1.0.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}

{
 "cells": [
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "# Using LineSearches without Optim/NLsolve"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This tutorial shows you how to use the line search algorithms in `LineSearches`\n",
    "for your own optimization algorithm that is not part of `Optim` or `NLsolve`.\n",
    "\n",
    "Say we have written a gradient descent optimization algorithm but would like to\n",
    "experiment with different line search algorithms.\n",
    "The algorithm is implemented as follows."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "gdoptimize (generic function with 4 methods)"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra: norm, dot\n",
    "\n",
    "function gdoptimize(f, g!, fg!, x0::AbstractArray{T}, linesearch,\n",
    "                    maxiter::Int = 10000,\n",
    "                    g_rtol::T = sqrt(eps(T)), g_atol::T = eps(T)) where T <: Number\n",
    "    x = copy(x0)\n",
    "    gvec = similar(x)\n",
    "    g!(gvec, x)\n",
    "    fx = f(x)\n",
    "\n",
    "    gnorm = norm(gvec)\n",
    "    gtol = max(g_rtol*gnorm, g_atol)\n",
    "\n",
    "    # Univariate line search functions\n",
    "    ϕ(α) = f(x .+ α.*s)\n",
    "    function dϕ(α)\n",
    "        g!(gvec, x .+ α.*s)\n",
    "        return dot(gvec, s)\n",
    "    end\n",
    "    function ϕdϕ(α)\n",
    "        phi = fg!(gvec, x .+ α.*s)\n",
    "        dphi = dot(gvec, s)\n",
    "        return (phi, dphi)\n",
    "    end\n",
    "\n",
    "    s = similar(gvec) # Step direction\n",
    "\n",
    "    iter = 0\n",
    "    while iter < maxiter && gnorm > gtol\n",
    "        iter += 1\n",
    "        s .= -gvec\n",
    "\n",
    "        dϕ_0 = dot(s, gvec)\n",
    "        α, fx = linesearch(ϕ, dϕ, ϕdϕ, 1.0, fx, dϕ_0)\n",
    "\n",
    "        @. x = x + α*s\n",
    "        g!(gvec, x)\n",
    "        gnorm = norm(gvec)\n",
    "    end\n",
    "\n",
    "    return (fx, x, iter)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Note that there are many optimization and line search algorithms that allow\n",
    "the user to evaluate both the objective and the gradient at the same time, for\n",
    "computational efficiency reasons.\n",
    "We have included this functionality in the algorithm as the input function `fg!`,\n",
    "and even if the Gradient Descent algorithm does not use it explicitly, many of the\n",
    "LineSearches algorithms do."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "The Gradient Descent `gdoptimize` method selects a descent direction and calls\n",
    "the line search algorithm  `linesearch` which returns the step length `α` and the\n",
    "objective value `fx = f(x + α*s)`.\n",
    "\n",
    "The functions ϕ and dϕ represent a univariate objective\n",
    "and its derivative, which is used by the line search algorithms.\n",
    "To utilize the `fg!` function call in the optimizer, some of the line searches\n",
    "require a function ϕdϕ which returns the univariate objective and the\n",
    "derivative at the same time."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Optimizing Rosenbrock\n",
    "Here is an example to show how we can combine `gdoptimize` and `LineSearches`\n",
    "to minimize the Rosenbrock function, which is defined by"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "fg! (generic function with 1 method)"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "\n",
    "function g!(gvec, x)\n",
    "    gvec[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "    gvec[2] = 200.0 * (x[2] - x[1]^2)\n",
    "    gvec\n",
    "end\n",
    "\n",
    "function fg!(gvec, x)\n",
    "    g!(gvec, x)\n",
    "    f(x)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We can now use `gdoptimize` with `BackTracking` to optimize the Rosenbrock function\n",
    "from a given initial condition `x0`."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2.0620997768295942e-15, [1.0, 1.0], 5759)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "x0 = [-1., 1.0]\n",
    "\n",
    "using LineSearches\n",
    "ls = BackTracking(order=3)\n",
    "fx_bt3, x_bt3, iter_bt3 = gdoptimize(f, g!, fg!, x0, ls)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Interestingly, the `StrongWolfe` line search converges in one iteration, whilst\n",
    "all the other algorithms take thousands of iterations.\n",
    "This is just luck due to the particular choice of initial condition"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(0.0, [1.0, 1.0], 1)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "ls = StrongWolfe()\n",
    "fx_sw, x_sw, iter_sw = gdoptimize(f, g!, fg!, x0, ls)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.4"
  },
  "kernelspec": {
   "name": "julia-1.0",
   "display_name": "Julia 1.0.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}
